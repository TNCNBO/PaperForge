$schema: ./schema/mcp-agent.config.schema.json
anthropic: null
default_search_server: tavily
document_segmentation:
  enabled: false
  size_threshold_chars: 50000
execution_engine: asyncio
logger:
  level: info
  path_settings:
    path_pattern: logs/mcp-agent-{unique_id}.jsonl
    timestamp_format: '%Y%m%d_%H%M%S'
    unique_id: timestamp
  progress_display: false
  transports:
  - console
  - file
mcp:
  servers:
    tavily:
      args:
      - tools/tavily_search_server.py
      command: python
      description: AI-optimized search engine for high-quality, relevant results
      env:
        TAVILY_API_KEY: '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'
        PYTHONPATH: .
    bocha-mcp:
      args:
      - tools/bocha_search_server.py
      command: python
      env:
        BOCHA_API_KEY: ''
        PYTHONPATH: .
    filesystem:
      # macos and linux should use this
      # args:
      # - -y
      # - '@modelcontextprotocol/server-filesystem'
      # - .
      # command: npx

      # windows should use this
      args:
      # please use the correct path for your system
      - E:\Develop\nodejs\npm\node_modules\@modelcontextprotocol\server-filesystem\dist\index.js
      - .
      command: node


    code-implementation:
      args:
      - tools/code_implementation_server.py
      command: python
      description: Paper code reproduction tool server - provides file operations,
        code execution, search and other functions
      env:
        PYTHONPATH: .
    code-reference-indexer:
      args:
      - tools/code_reference_indexer.py
      command: python
      description: Code reference indexer server - Provides intelligent code reference
        search from indexed repositories
      env:
        PYTHONPATH: .
    command-executor:
      args:
      - tools/command_executor.py
      command: python
      env:
        PYTHONPATH: .
    document-segmentation:
      args:
      - tools/document_segmentation_server.py
      command: python
      description: Document segmentation server - Provides intelligent document analysis
        and segmented reading to optimize token usage
      env:
        PYTHONPATH: .
    fetch:
      args:
      - mcp-server-fetch
      command: uvx
    file-downloader:
      args:
      - tools/pdf_downloader.py
      command: python
      env:
        PYTHONPATH: .
    github-downloader:
      args:
      - tools/git_command.py
      command: python
      env:
        PYTHONPATH: .
# LLM Provider Priority (选择使用哪个LLM / Choose which LLM to use)
# Options: "anthropic", "google", "openai"
# If not set or provider unavailable, will fallback to first available provider
llm_provider: "openai"  # 使用 OpenAI 兼容接口（硅基流动）

openai:
  base_max_tokens: 40000
  # 硅基流动支持的模型示例：
  default_model: "deepseek-ai/DeepSeek-V3"  # DeepSeek V3
  # default_model: "deepseek-ai/DeepSeek-V2.5"  # DeepSeek V2.5
  # default_model: "THUDM/glm-4-9b-chat"  # GLM-4 9B
  # default_model: "meta-llama/Meta-Llama-3.1-70B-Instruct"  # Llama 3.1 70B
  # default_model: "google/gemini-2.5-pro"  # 原始配置
  # default_model: anthropic/claude-sonnet-4.5
  reasoning_effort: low  # Only for thinking models
  max_tokens_policy: adaptive
  retry_max_tokens: 32768

# Configuration for Google AI (Gemini)
google:
  default_model: "gemini-3-pro-preview"

anthropic:
  default_model: "claude-sonnet-4.5"

planning_mode: traditional
